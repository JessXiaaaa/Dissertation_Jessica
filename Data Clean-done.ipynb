{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import water quality data \n",
    "data = pd.read_csv('2017 Water Quality Archive.csv') \n",
    "\n",
    "# Check dataframe head \n",
    "print(\"Dataframe head：\")\n",
    "print(data.head(),\"\\n\")\n",
    "\n",
    "# Check dataframe shape \n",
    "print(\"Dataframe shape ：\")\n",
    "print(data.shape,\"\\n\")\n",
    "\n",
    "# Check data types and not Null value\n",
    "print(\"Data types and not Null value：\")\n",
    "print(data.info(),\"\\n\")\n",
    "\n",
    "# Check for missing values \n",
    "print(\"Missing values：\")\n",
    "print(data.isnull().sum(),\"\\n\")\n",
    "\n",
    "#Check uniqueness of columns \n",
    "print(\"Number of unique values per column：\")\n",
    "print(data.nunique(),\"\\n\")\n",
    "\n",
    "# Check for duplicate rows \n",
    "print(data[data.duplicated()].shape[0], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Specify the range of years to be processed\n",
    "years = range(2000, 2023)  # From year 2000 to 2022\n",
    "\n",
    "for year in years:\n",
    "    input_file_name = f\"{year} Water Quality Archive.csv\"\n",
    "    output_file_name = f\"{year} Water Quality Archive Cleaned.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_file_name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {input_file_name} not found. Skipping this year.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {input_file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"{input_file_name} shape before processing: {df.shape}\")\n",
    "\n",
    "    # Check and remove rows with non-NA 'deviating_result' values if the column exists\n",
    "    if 'deviating_result' in df.columns:\n",
    "        initial_count = df.shape[0]\n",
    "        df = df[df['deviating_result'].isna()]\n",
    "        final_count = df.shape[0]\n",
    "        print(f\"Removed {initial_count - final_count} rows with non-NA 'deviating_result'\")\n",
    "    else:\n",
    "        print(\"'deviating_result' column not found in the data.\")\n",
    "\n",
    "    # Remove specific columns and handle their absence gracefully\n",
    "    for col in ['deviating_result', 'sign', 'unit_name']:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "        else:\n",
    "            print(f\"{col} column not present in {input_file_name}.\")\n",
    "\n",
    "    # Attempt to convert 'parameter_shortname' to int and handle errors\n",
    "    if 'parameter_shortname' in df.columns:\n",
    "        try:\n",
    "            df['parameter_shortname'] = df['parameter_shortname'].astype(int)\n",
    "        except ValueError:\n",
    "            print(f\"Cannot convert 'parameter_shortname' to int in {input_file_name}\")\n",
    "\n",
    "    # Handling 'unit_symbol' replacement\n",
    "    if 'unit_symbol' in df.columns:\n",
    "        df['unit_symbol'] = df['unit_symbol'].replace('---', 'scalar')\n",
    "    else:\n",
    "        print(\"'unit_symbol' column not found in the data.\")\n",
    "\n",
    "    # Handling 'sample_value' conversions and deletions for erroneous entries\n",
    "    if 'sample_value' in df.columns and df['sample_value'].dtype == object:\n",
    "        original_sample_values = df['sample_value'].copy()  # Save original values for comparison\n",
    "        df['sample_value'] = pd.to_numeric(df['sample_value'], errors='coerce')\n",
    "        df.dropna(subset=['sample_value'], inplace=True)\n",
    "        print(f\"Processed 'sample_value' conversions. Data type: {df['sample_value'].dtype}\")\n",
    "\n",
    "    print(f\"{input_file_name} shape after processing: {df.shape}\")\n",
    "    try:\n",
    "        df.to_csv(output_file_name, index=False)  # Save the modified data to a new file\n",
    "        print(f\"{output_file_name} processed and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save {output_file_name}: {e}\")\n",
    "\n",
    "    print(f\"Data types after processing {output_file_name}:\\n{df.dtypes}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reads water quality data files from 2000 to 2023, provides an overview of each dataset,\n",
    "# and collects summary statistics such as the number of rows, columns, missing values, and unique values.\n",
    "# It then stores the summary information in a DataFrame for further analysis or visualization.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "years = range(2000, 2024)\n",
    "data_summary = []\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    df = pd.read_csv(f'{year} Water Quality Archive.csv')\n",
    "    \n",
    "    # Optionally, compare with the cleaned data\n",
    "    # df = pd.read_csv(f'{year} Water Quality Archive Cleaned.csv')\n",
    "    \n",
    "    # Print basic information for each year's dataset\n",
    "    print(f\"Year: {year}\")\n",
    "    print(\"Head of data:\\n\", df.head())           # Display first few rows\n",
    "    print(\"Shape of data:\", df.shape)             # Show number of rows and columns\n",
    "    print(\"Data types:\\n\", df.dtypes)             # Show column data types\n",
    "    \n",
    "    # Calculate missing values for each column\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Missing values:\\n\", missing_values)\n",
    "    \n",
    "    # Calculate the number of unique values for each column\n",
    "    unique_values = df.nunique()\n",
    "    print(\"Number of unique values per column:\\n\", unique_values)\n",
    "    \n",
    "    # Append summary information (year, rows, columns, missing and unique values) to the list\n",
    "    data_summary.append({\n",
    "        'Year': year,\n",
    "        'Rows': df.shape[0],\n",
    "        'Columns': df.shape[1],\n",
    "        'Missing Values': missing_values.sum(),\n",
    "        'Unique Values': unique_values.sum()\n",
    "    })\n",
    "\n",
    "# Convert the collected summary data into a DataFrame for further use or visualization\n",
    "summary_df = pd.DataFrame(data_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script processes water quality data files from 2000 to 2023. \n",
    "# It checks for consistency in the column names and count across all the years, comparing them with the first year as a baseline.\n",
    "# Additionally, it identifies and summarizes any missing values in the columns for each year.\n",
    "# The script outputs any discrepancies in column structure and missing value information, providing a comprehensive overview of the data quality.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the range of years to be processed\n",
    "years = range(2000, 2024)\n",
    "\n",
    "# Initialize variables to store column information for the first year for comparison\n",
    "initial_columns = None\n",
    "initial_column_count = None\n",
    "column_consistency = True\n",
    "\n",
    "# Dictionary to store missing values information across years\n",
    "missing_values_summary = {}\n",
    "\n",
    "# Process each file\n",
    "for year in years:\n",
    "    file_name = f'{year} Water Quality Archive Cleaned.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found. Skipping this year.\")\n",
    "        continue\n",
    "\n",
    "    # Retrieve current year column information\n",
    "    current_columns = df.columns\n",
    "    current_column_count = len(current_columns)\n",
    "\n",
    "    # Initialize column comparison in the first year\n",
    "    if initial_columns is None:\n",
    "        initial_columns = current_columns\n",
    "        initial_column_count = current_column_count\n",
    "        print(f\"Baseline set with {initial_column_count} columns from the year {year}.\")\n",
    "    else:\n",
    "        # Compare the number of columns and the column names with the baseline year\n",
    "        if current_column_count != initial_column_count or set(current_columns) != set(initial_columns):\n",
    "            column_consistency = False\n",
    "            print(f\"Year {year}: Column discrepancies detected.\")\n",
    "            if current_column_count != initial_column_count:\n",
    "                print(f\"  - Different number of columns: {current_column_count} (expected {initial_column_count}).\")\n",
    "            if set(current_columns) != set(initial_columns):\n",
    "                print(f\"  - Different column names.\")\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_values = df.isna().sum()\n",
    "    for column, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            if column not in missing_values_summary:\n",
    "                missing_values_summary[column] = {}\n",
    "            missing_values_summary[column][year] = count\n",
    "\n",
    "    #print()  # Add a space for better readability between years\n",
    "\n",
    "# Check if all years had the same columns if no discrepancies were noted\n",
    "if column_consistency:\n",
    "    print(\"All years have consistent columns.\")\n",
    "\n",
    "# Print the summary of missing values\n",
    "if missing_values_summary:\n",
    "    print(\"Summary of Missing Values by Column Across All Years:\")\n",
    "    for column, details in missing_values_summary.items():\n",
    "        print(f\"Column: {column}\")\n",
    "        for year, count in details.items():\n",
    "            print(f\"  Year {year}: {count} missing values\")\n",
    "else:\n",
    "    print(\"No missing values found across any year.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the range of years for the analysis (from 2000 to 2023)\n",
    "years = range(2000, 2024)\n",
    "\n",
    "# Define the columns to be analyzed for unique values\n",
    "columns_to_analyze = [\n",
    "    'station_type', 'local_authority', 'sampling_reason', 'reason_group', 'sampling_medium', \n",
    "    'sampling_mechanism', 'parameter_shortname', 'parameter_name', 'method_name',\n",
    "    'coded_value'\n",
    "]\n",
    "\n",
    "# Create a dictionary to store the unique values for each column\n",
    "unique_values = {col: set() for col in columns_to_analyze}\n",
    "\n",
    "# Loop through each year to process the respective file\n",
    "for year in years:\n",
    "    # Construct the file name and read the data\n",
    "    file_name = f'{year} Water Quality Archive Cleaned.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_name)\n",
    "        \n",
    "        # Update the unique values set for each column\n",
    "        for col in columns_to_analyze:\n",
    "            if col in df.columns:\n",
    "                unique_values[col].update(df[col].dropna().unique())\n",
    "            else:\n",
    "                # Print a message if the column is not found in the current file\n",
    "                print(f\"Column {col} not found in {file_name}\")\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the file does not exist\n",
    "        print(f\"File {file_name} not found. Skipping this year.\")\n",
    "\n",
    "# Prepare the results for saving as a CSV file\n",
    "output_data = {}\n",
    "for col, values in unique_values.items():\n",
    "    output_data[col] = list(values)\n",
    "\n",
    "# Convert the dictionary of unique values to a DataFrame\n",
    "output_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in output_data.items()]))\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv('unique_values_summary.csv', index=False)\n",
    "\n",
    "# Print the number of unique values and the unique values for each column\n",
    "for col, values in unique_values.items():\n",
    "    print(\"\\n\")  # Ensure space between each column's output\n",
    "    print(f\"Column '{col}' has {len(values)} unique values.\")\n",
    "    print(f\"Unique values for '{col}': {values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re  \n",
    "\n",
    "\n",
    "years = range(2000, 2024)\n",
    "# Dictionary to map specific station types to broader categories, ensuring all text is lowercase\n",
    "category_map = {\n",
    "\n",
    "\n",
    "    'Waste Sites': [\n",
    "        'waste site leachate borehole', 'waste site gas groundwater', 'waste site domestic industrial landfill',\n",
    "        'waste site unspecified', 'waste site external standing water inc ponds ditches', 'waste site treated effluent',\n",
    "        'waste site gas borehole', 'waste site leachate management system', 'waste site surface water monitoring point landfill'\n",
    "    ],\n",
    "    'Trade Discharges': [\n",
    "        'trade discharges process effluent not water company', 'trade discharges mineral workings',\n",
    "        'trade discharges process effluent water company wtw', 'trade discharges site drainage contam surface water not waste sit',\n",
    "        'trade discharges site drainage', 'trade discharges abandoned', 'trade discharges cooling water', 'trade discharges unspecified',\n",
    "        'trade release to air'\n",
    "    ],\n",
    "    'Freshwater Sites': [\n",
    "        'freshwater canals non classified', 'freshwater rqo re1', 'freshwater comparative inlet points', 'freshwater land drains',\n",
    "        'freshwater lakes ponds reservoirs', 'freshwater non classified river points', 'freshwater unspecified', 'freshwater rqo re2',\n",
    "        'freshwater rqo re3', 'freshwater bathing water'\n",
    "    ],\n",
    "    'Sewage Discharges': [\n",
    "        'sewage discharges unspecified water company', 'sewage crude to further treatment water company', \n",
    "        'sewage discharges stw storm overflow storm tank not water company', 'sewage discharges sludge water company',\n",
    "        'sewage discharges stw storm overflow storm tank water company', 'sewage discharges final treated effluent not water company',\n",
    "        'sewage discharges final treated effluent water company', 'sewage discharges pumping station water company', \n",
    "        'sewage trade combined unspecified', 'sewage discharges sewer storm overflow water company', \n",
    "        'sewage discharges unspecified not water company', 'sewage discharges sewer storm overflow not water company',\n",
    "        'sewerage system discharge'\n",
    "    ],\n",
    "    'Saline Water Sites': [\n",
    "        'saline water designated shellfisheries', 'saline water estuarine sites corrected',\n",
    "        'saline water estuary class c', 'saline water non designated bathing beaches', 'saline water estuarine sites non bathing shellfish',\n",
    "        'saline water estuary class a', 'saline water comparative inlet point', 'saline water designated bathing beaches',\n",
    "        'saline water coastal sites non bathing shellfish', 'saline water estuary class b'\n",
    "    ],\n",
    "    'Agriculture Sites': [\n",
    "        'agriculture unspecified', 'agriculture site drainage', 'agriculture fish farming not water company',\n",
    "        'agriculture fish farming water company'\n",
    "    ],\n",
    "    'Miscellaneous Sites': [\n",
    "        'miscellaneous environment sediments', 'miscellaneous discharges surface water',\n",
    "        'miscellaneous discharges highway drainage', 'miscellaneous environment unspecified', \n",
    "        'miscellaneous discharges unspecified', 'miscellaneous environment soils', 'miscellaneous discharges mine groundwater as raised'\n",
    "    ],\n",
    "    'Other Categories': [\n",
    "        'pollution investigation points environment', 'rainwater', 'unspecified'\n",
    "    ],\n",
    "    'Minewater': [\n",
    "        'minewater'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9]+', ' ', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "# Function to map station types to broader categories\n",
    "def map_category(station_type):\n",
    "    cleaned_type = clean_text(station_type)\n",
    "    for category, types in category_map.items():\n",
    "        if cleaned_type in types:\n",
    "            return category\n",
    "            \n",
    "    print(f\"Warning: '{station_type}' did not match any category.\")  \n",
    "    return 'Other Categories'\n",
    "\n",
    "  \n",
    "# List to store data frames for each year\n",
    "data_frames = []\n",
    "\n",
    "# Loop through each year and read the data files\n",
    "for year in years:\n",
    "    file_name = f'{year} Water Quality Archive Cleaned.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_name)\n",
    "        if 'station_number' in df.columns and 'station_type' in df.columns:\n",
    "            # Clean station_type text and map to category\n",
    "            df['station_type'] = df['station_type'].astype(str).apply(clean_text)\n",
    "            df['category'] = df['station_type'].apply(map_category)\n",
    "            df = df[['station_number', 'category']]  # Keep relevant columns\n",
    "            df['year'] = year  # Add year column\n",
    "            data_frames.append(df)  # Append to the list\n",
    "        else:\n",
    "            print(f\"Required columns are missing in {file_name}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found. Skipping this year.\")\n",
    "\n",
    "# Combine all yearly data frames into a single data frame\n",
    "full_data = pd.concat(data_frames)\n",
    "\n",
    "# Count the number of stations per category for each year\n",
    "category_counts = full_data.groupby(['year', 'category']).size().unstack(fill_value=0)\n",
    "\n",
    "# Save the result as a CSV file\n",
    "category_counts.to_csv('station_type_category_counts_by_year.csv')\n",
    "\n",
    "# Plotting the change in station counts over the years\n",
    "plt.figure(figsize=(20, 10))\n",
    "for category in category_counts.columns:\n",
    "    plt.plot(category_counts.index, category_counts[category], marker='o', label=category)\n",
    "\n",
    "# Add labels, title, and legend to the plot\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Stations')\n",
    "plt.title('Change in Number of Stations by Category (2000-2023)')\n",
    "plt.legend(title='Station Type Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the aggregated category counts from the CSV file\n",
    "category_counts = pd.read_csv('station_type_category_counts_by_year.csv', index_col='year')\n",
    "\n",
    "# Separate data into different category groups for plotting\n",
    "freshwater_counts = category_counts['Freshwater Sites']\n",
    "saline_sewage_counts = category_counts[['Saline Water Sites', 'Sewage Discharges']]\n",
    "waste_counts = category_counts['Waste Sites']\n",
    "agriculture_other_counts = category_counts[['Agriculture Sites', 'Other Categories']]\n",
    "other_counts = category_counts.drop(columns=['Freshwater Sites', 'Saline Water Sites', 'Sewage Discharges', \n",
    "                                            'Waste Sites', 'Agriculture Sites', 'Other Categories'])\n",
    "\n",
    "# Plot the number of Freshwater Sites over the years\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(freshwater_counts.index, freshwater_counts, marker='o', label='Freshwater Sites', color='blue')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Stations')\n",
    "plt.title('Change in Number of Freshwater Sites (2000-2023)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot the number of Saline Water Sites and Sewage Discharges over the years\n",
    "plt.figure(figsize=(20, 6))\n",
    "for category in saline_sewage_counts.columns:\n",
    "    plt.plot(saline_sewage_counts.index, saline_sewage_counts[category], marker='o', label=category)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Stations')\n",
    "plt.title('Change in Number of Saline Water Sites and Sewage Discharges (2000-2023)')\n",
    "plt.legend(title='Station Type Category', loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot the number of Waste Sites over the years\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(waste_counts.index, waste_counts, marker='o', label='Waste Sites', color='green')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Stations')\n",
    "plt.title('Change in Number of Waste Sites (2000-2023)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot the number of Agriculture Sites and Other Categories over the years\n",
    "plt.figure(figsize=(20, 6))\n",
    "for category in agriculture_other_counts.columns:\n",
    "    plt.plot(agriculture_other_counts.index, agriculture_other_counts[category], marker='o', label=category)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Stations')\n",
    "plt.title('Change in Number of Agriculture Sites and Other Categories (2000-2023)')\n",
    "plt.legend(title='Station Type Category', loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot the number of stations in other categories over the years\n",
    "plt.figure(figsize=(20, 10))\n",
    "for category in other_counts.columns:\n",
    "    plt.plot(other_counts.index, other_counts[category], marker='o', label=category)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Stations')\n",
    "plt.title('Change in Number of Stations by Other Categories (2000-2023)')\n",
    "plt.legend(title='Station Type Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings to prevent unnecessary output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize dictionaries to store station information and stations with one-to-many mappings\n",
    "total_station_info = {}\n",
    "total_one_to_many = {}\n",
    "\n",
    "# Define the range of years for which data will be processed\n",
    "years = range(2000, 2024)\n",
    "\n",
    "# Loop through each year and process the data\n",
    "for year in years:\n",
    "    df = pd.read_csv(f'{year} Water Quality Archive Cleaned.csv')\n",
    "    \n",
    "    # Group the data by station number and check for duplicates in specific columns\n",
    "    for station_number, group in df.groupby('station_number'):\n",
    "        if len(group[['station_name', 'station_type', 'northing', 'easting', 'ngr']].drop_duplicates()) == 1:\n",
    "            # If no duplicates, store or update the station information\n",
    "            row = group.iloc[0]\n",
    "            total_station_info[station_number] = {\n",
    "                'Station Name': row['station_name'],\n",
    "                'Station Type': row['station_type'],\n",
    "                'Northing': row['northing'],\n",
    "                'Easting': row['easting'],\n",
    "                'ngr': row['ngr']\n",
    "            }\n",
    "        else:\n",
    "            # If duplicates exist (one-to-many mapping), add the station number to the one-to-many dictionary\n",
    "            if station_number not in total_one_to_many:\n",
    "                total_one_to_many[station_number] = []\n",
    "            total_one_to_many[station_number].append(year)\n",
    "\n",
    "# Save the station information as a CSV file, overwriting the existing file if it exists\n",
    "station_info_df = pd.DataFrame.from_dict(total_station_info, orient='index')\n",
    "station_info_df.to_csv('total_station_info.csv', mode='w')\n",
    "\n",
    "# Save the one-to-many mapping as a separate CSV file\n",
    "one_to_many_df = pd.DataFrame([(station, years) for station, years in total_one_to_many.items()],\n",
    "                              columns=['Station Number', 'Years'])\n",
    "one_to_many_df.to_csv('total_one_to_many.csv', mode='w', index=False)\n",
    "\n",
    "# Print the number of entries in both data sets for confirmation\n",
    "station_info_count = len(station_info_df)\n",
    "print(f\"Total entries in station_info: {station_info_count}\")\n",
    "\n",
    "one_to_many_count = len(one_to_many_df)\n",
    "print(f\"\\nTotal entries in one_to_many: {one_to_many_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Read data from the 'total_station_info.csv' file\n",
    "station_info_df = pd.read_csv('total_station_info.csv', index_col=0)\n",
    "\n",
    "# Sort the DataFrame by 'Station Name' in ascending order (A-Z)\n",
    "sorted_station_info_df = station_info_df.sort_values(by='Station Name')\n",
    "\n",
    "# Print the total number of entries in the sorted DataFrame\n",
    "print(f\"Total entries in station_info: {len(sorted_station_info_df)}\")\n",
    "\n",
    "# Display the sorted DataFrame as an HTML table with the index column visible\n",
    "print(\"Sorted Station Info Table by Station Name (A-Z):\")\n",
    "display(HTML(sorted_station_info_df.to_html(index=True)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Read the saved CSV file containing station information\n",
    "station_info_df = pd.read_csv('total_station_info.csv', index_col=0)\n",
    "\n",
    "# Group the data by 'Northing' and 'Easting' to calculate the number of stations at each location\n",
    "location_groups = station_info_df.groupby(['Northing', 'Easting']).size()\n",
    "\n",
    "# Filter the groups where the location has more than one station (duplicates)\n",
    "duplicates = location_groups[location_groups > 1]\n",
    "\n",
    "# Display the results\n",
    "print(\"Duplicate Locations (Northing and Easting with more than one station):\")\n",
    "if not duplicates.empty:\n",
    "    for location, count in duplicates.items():\n",
    "        print(f\"Location (Northing: {location[0]}, Easting: {location[1]}) - Stations: {count}\")\n",
    "        # Display the station details for these duplicate locations in HTML format\n",
    "        display(HTML(station_info_df[(station_info_df['Northing'] == location[0]) & \n",
    "                                     (station_info_df['Easting'] == location[1])].to_html(index=False)))\n",
    "else:\n",
    "    print(\"No duplicate locations found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Load the CSV file containing station information\n",
    "station_info_df = pd.read_csv('total_station_info.csv', index_col=0)\n",
    "\n",
    "# Group the data by 'Northing' and 'Easting' to count how many stations are located at each unique position\n",
    "location_groups = station_info_df.groupby(['Northing', 'Easting']).size()\n",
    "\n",
    "# Filter out locations where more than one station exists (duplicates)\n",
    "duplicates = location_groups[location_groups > 1]\n",
    "\n",
    "# Initialize an empty DataFrame to store details about stations at duplicate locations\n",
    "all_duplicate_info_df = pd.DataFrame()\n",
    "\n",
    "# Display and collect duplicate station details\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicate Locations (Northing and Easting with more than one station):\")\n",
    "    for location, count in duplicates.items():\n",
    "        print(f\"Location (Northing: {location[0]}, Easting: {location[1]}) - Stations: {count}\")\n",
    "        # Extract the stations for the given duplicate location\n",
    "        stations_at_location = station_info_df[(station_info_df['Northing'] == location[0]) & \n",
    "                                               (station_info_df['Easting'] == location[1])]\n",
    "        # Display the station details in an HTML table\n",
    "        display(HTML(stations_at_location.to_html(index=False)))\n",
    "        # Append the duplicate station details to the DataFrame\n",
    "        all_duplicate_info_df = pd.concat([all_duplicate_info_df, stations_at_location], ignore_index=True)\n",
    "else:\n",
    "    print(\"No duplicate locations found.\")\n",
    "\n",
    "# Save the collected duplicate station information to a new CSV file\n",
    "all_duplicate_info_df.to_csv('all_duplicate_stations_info.csv', index=False)\n",
    "\n",
    "print(\"All duplicate station information has been saved to 'all_duplicate_stations_info.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file containing station information\n",
    "station_info_df = pd.read_csv('total_station_info.csv', index_col=0)\n",
    "\n",
    "# Standardize the 'Station Type' column by stripping leading/trailing spaces and converting text to lowercase\n",
    "station_info_df['Station Type'] = station_info_df['Station Type'].str.strip().str.lower()\n",
    "\n",
    "# Filter the DataFrame to select only rows where 'Station Type' contains 'minewater'\n",
    "minewater_stations_df = station_info_df[station_info_df['Station Type'].str.contains('minewater', na=False)]\n",
    "\n",
    "# Save the filtered minewater stations information to a new CSV file\n",
    "minewater_stations_df.to_csv('minewater_stations_info.csv', index=False)\n",
    "\n",
    "# Display the filtered minewater stations information\n",
    "print(\"Minewater Stations Information:\")\n",
    "print(minewater_stations_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any mismatches between these two columns\n",
    "parameter_check = data[['parameter_shortname', 'parameter_name']].drop_duplicates()\n",
    "print(\"Unique Parameter Check:\", parameter_check.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define a text cleaning function to remove special characters and normalize spaces\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9]+', ' ', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "# Set the range of years to process\n",
    "years = range(2000, 2023 + 1)\n",
    "\n",
    "# Initialize Series to store overall counts for 'sampling_reason' and 'reason_group'\n",
    "overall_sampling_reason_counts = pd.Series(dtype=int)\n",
    "overall_reason_group_counts = pd.Series(dtype=int)\n",
    "\n",
    "# Loop through each year's file and process it\n",
    "for year in years:\n",
    "    file_name = f'{year} Water Quality Archive Cleaned.csv'\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_name)\n",
    "        \n",
    "        # Check if the necessary columns exist\n",
    "        if 'sampling_reason' in df.columns and 'reason_group' in df.columns:\n",
    "            # Clean the text data in the relevant columns\n",
    "            df['sampling_reason'] = df['sampling_reason'].apply(clean_text)\n",
    "            df['reason_group'] = df['reason_group'].apply(clean_text)\n",
    "\n",
    "            # Count the occurrences of each 'sampling_reason' and 'reason_group'\n",
    "            sampling_reason_counts = df['sampling_reason'].value_counts()\n",
    "            reason_group_counts = df['reason_group'].value_counts()\n",
    "\n",
    "            # Add the counts to the overall totals\n",
    "            overall_sampling_reason_counts = overall_sampling_reason_counts.add(sampling_reason_counts, fill_value=0)\n",
    "            overall_reason_group_counts = overall_reason_group_counts.add(reason_group_counts, fill_value=0)\n",
    "        \n",
    "        else:\n",
    "            print(f\"File {file_name} missing required columns, skipping...\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found, skipping...\")\n",
    "\n",
    "# Save the total counts to CSV files\n",
    "overall_sampling_reason_counts.to_csv('total_sampling_reason_counts_2000_2023.csv')\n",
    "overall_reason_group_counts.to_csv('total_reason_group_counts_2000_2023.csv')\n",
    "\n",
    "print(\"Yearly data has been processed, summed, and saved.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# This script processes water quality data from multiple CSV files, \n",
    "# cleans the text data, performs statistical analysis, and saves the results.\n",
    "\n",
    "# Define a function to clean text data\n",
    "def clean_text(text):\n",
    "    try:\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9]+', ' ', text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "        return cleaned_text.lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning text: {e}\")\n",
    "        return text\n",
    "\n",
    "# Set the range of years to process\n",
    "years = range(2000, 2023 + 1)\n",
    "\n",
    "# Initialize Series to store overall counts\n",
    "overall_sampling_reason_counts = pd.Series(dtype='int32')\n",
    "overall_reason_group_counts = pd.Series(dtype='int32')\n",
    "\n",
    "# Initialize a DataFrame to accumulate all files' data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Read and process each file for the specified years\n",
    "for year in years:\n",
    "    file_name = f'{year} Water Quality Archive Cleaned.csv'\n",
    "    \n",
    "    try:\n",
    "        # Load the data with specified data types to optimize memory usage\n",
    "        df = pd.read_csv(file_name, dtype={'sampling_reason': 'category', 'reason_group': 'category'})\n",
    "        \n",
    "        if 'sampling_reason' in df.columns and 'reason_group' in df.columns:\n",
    "            try:\n",
    "                # Clean the text columns\n",
    "                df['sampling_reason'] = df['sampling_reason'].apply(clean_text)\n",
    "                df['reason_group'] = df['reason_group'].apply(clean_text)\n",
    "\n",
    "                # Calculate counts for the current year\n",
    "                sampling_reason_counts = df['sampling_reason'].value_counts().astype('int32')\n",
    "                reason_group_counts = df['reason_group'].value_counts().astype('int32')\n",
    "\n",
    "                # Accumulate the counts into overall statistics\n",
    "                overall_sampling_reason_counts = overall_sampling_reason_counts.add(sampling_reason_counts, fill_value=0).astype('int32')\n",
    "                overall_reason_group_counts = overall_reason_group_counts.add(reason_group_counts, fill_value=0).astype('int32')\n",
    "\n",
    "                # Add the current year's data to the combined DataFrame\n",
    "                combined_df = pd.concat([combined_df, df[['sampling_reason', 'reason_group']]], ignore_index=True)\n",
    "\n",
    "                # Release memory\n",
    "                del df, sampling_reason_counts, reason_group_counts\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed data for year {year}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing data for year {year}: {e}\")\n",
    "        else:\n",
    "            print(f\"File {file_name} missing required columns, skipping...\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found, skipping...\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"File {file_name} is empty, skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_name}: {e}\")\n",
    "\n",
    "# Save overall counts as CSV files\n",
    "try:\n",
    "    overall_sampling_reason_counts.to_csv('total_sampling_reason_counts_2000_2023.csv')\n",
    "    print(\"Saved total_sampling_reason_counts_2000_2023.csv successfully.\")\n",
    "\n",
    "    overall_reason_group_counts.to_csv('total_reason_group_counts_2000_2023.csv')\n",
    "    print(\"Saved total_reason_group_counts_2000_2023.csv successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving total counts: {e}\")\n",
    "\n",
    "# Create and save a cross-tabulation of the data\n",
    "try:\n",
    "    cross_tab = pd.crosstab(combined_df['sampling_reason'], combined_df['reason_group'])\n",
    "\n",
    "    cross_tab.to_csv('cross_tab_sampling_reason_vs_reason_group.csv')\n",
    "    print(\"Saved cross_tab_sampling_reason_vs_reason_group.csv successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating or saving cross tabulation: {e}\")\n",
    "\n",
    "# Perform chi-squared test and save the results\n",
    "try:\n",
    "    chi2, p, dof, expected = chi2_contingency(cross_tab)\n",
    "\n",
    "    with open('chi_squared_test_results.txt', 'w') as f:\n",
    "        f.write(f\"Chi2 Statistic: {chi2:.2f}\\n\")\n",
    "        f.write(f\"P-value: {p:.4f}\\n\")\n",
    "        f.write(f\"Degrees of Freedom: {dof}\\n\")\n",
    "        f.write(\"Expected Frequencies Table:\\n\")\n",
    "        f.write(pd.DataFrame(expected, index=cross_tab.index, columns=cross_tab.columns).to_string())\n",
    "    print(\"Saved chi_squared_test_results.txt successfully.\")\n",
    "\n",
    "    pd.DataFrame(expected, index=cross_tab.index, columns=cross_tab.columns).to_csv('expected_frequencies.csv')\n",
    "    print(\"Saved expected_frequencies.csv successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during chi-squared test or saving results: {e}\")\n",
    "\n",
    "# Print final results\n",
    "print(\"Yearly data has been processed, summed, and saved.\")\n",
    "print(\"\\nChi-squared Test Results:\")\n",
    "print(f\"Chi2 Statistic: {chi2:.2f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")\n",
    "\n",
    "# Interpretation of chi-squared test results\n",
    "if p < 0.05:\n",
    "    print(\"The relationship between sampling_reason and reason_group is statistically significant.\")\n",
    "else:\n",
    "    print(\"No significant relationship between sampling_reason and reason_group.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This script loads a contingency table from a CSV file, \n",
    "# visualizes it as a heatmap, and then computes and visualizes \n",
    "# the residuals (observed - expected frequencies) from a chi-squared test.\n",
    "\n",
    "# 1. Load the contingency table from a CSV file\n",
    "cross_tab = pd.read_csv('cross_tab_sampling_reason_vs_reason_group.csv', index_col=0)\n",
    "\n",
    "# 2. Visualization: Heatmap of the contingency table\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cross_tab, annot=True, cmap='YlGnBu', fmt='d', cbar=True)\n",
    "plt.title('Heatmap of Sampling Reason vs Reason Group')\n",
    "plt.xlabel('Reason Group')\n",
    "plt.ylabel('Sampling Reason')\n",
    "plt.show()\n",
    "\n",
    "# 3. Load expected frequencies from the generated CSV file\n",
    "expected = pd.read_csv('expected_frequencies.csv', index_col=0)\n",
    "\n",
    "# 4. Calculate the residuals (observed - expected values)\n",
    "residuals = cross_tab - expected\n",
    "\n",
    "# 5. Visualization: Heatmap of the residuals\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(residuals, annot=True, cmap='RdBu_r', center=0, cbar=True)\n",
    "plt.title('Residuals Heatmap: Observed - Expected Frequencies')\n",
    "plt.xlabel('Reason Group')\n",
    "plt.ylabel('Sampling Reason')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# This script reads total sampling reason counts, separates them based on frequency, \n",
    "# and visualizes high and low frequency counts using bar plots.\n",
    "\n",
    "# Read overall sampling reason counts from the CSV file\n",
    "overall_sampling_reason_counts = pd.read_csv('total_sampling_reason_counts_2000_2023.csv', index_col=0, squeeze=True)\n",
    "\n",
    "# Separate counts into high and low frequency based on a threshold of 300,000\n",
    "high_sampling_reason_counts = overall_sampling_reason_counts[overall_sampling_reason_counts >= 300000]\n",
    "low_sampling_reason_counts = overall_sampling_reason_counts[overall_sampling_reason_counts < 300000]\n",
    "\n",
    "# Plot bar chart for high frequency sampling reasons (>= 300,000)\n",
    "plt.figure(figsize=(14, 7))\n",
    "high_sampling_reason_counts.plot(kind='bar', color=sns.color_palette('Paired', len(high_sampling_reason_counts)))\n",
    "plt.title('Sampling Reason Counts (2000-2023) - High Frequency (>= 300,000)')\n",
    "plt.xlabel('Sampling Reason')\n",
    "plt.ylabel('Total Count')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot bar chart for low frequency sampling reasons (< 300,000)\n",
    "plt.figure(figsize=(14, 7))\n",
    "low_sampling_reason_counts.plot(kind='bar', color=sns.color_palette('Set3', len(low_sampling_reason_counts)))\n",
    "plt.title('Sampling Reason Counts (2000-2023) - Low Frequency (< 300,000)')\n",
    "plt.xlabel('Sampling Reason')\n",
    "plt.ylabel('Total Count')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# This script reads total reason group counts, prints the distribution for analysis, \n",
    "# and visualizes high and low frequency counts using bar plots.\n",
    "\n",
    "# Read overall reason group counts from the CSV file\n",
    "overall_reason_group_counts = pd.read_csv('total_reason_group_counts_2000_2023.csv', index_col=0, squeeze=True)\n",
    "\n",
    "# Print the sorted distribution of reason group counts to observe and decide on thresholds\n",
    "print(overall_reason_group_counts.sort_values(ascending=False))\n",
    "\n",
    "# Divide the counts into high and low frequency based on a threshold of 1,000,000\n",
    "high_reason_group_counts = overall_reason_group_counts[overall_reason_group_counts >= 1000000]\n",
    "low_reason_group_counts = overall_reason_group_counts[overall_reason_group_counts < 1000000]\n",
    "\n",
    "# Plot bar chart for high frequency reason groups (>= 1,000,000)\n",
    "plt.figure(figsize=(10, 6))\n",
    "high_reason_group_counts.plot(kind='bar', color=sns.color_palette('Set2', len(high_reason_group_counts)))\n",
    "plt.title('Reason Group Counts (2000-2023) - High Frequency (>= 1,000,000)')\n",
    "plt.xlabel('Reason Group')\n",
    "plt.ylabel('Total Count')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot bar chart for low frequency reason groups (< 1,000,000)\n",
    "plt.figure(figsize=(10, 6))\n",
    "low_reason_group_counts.plot(kind='bar', color=sns.color_palette('Set3', len(low_reason_group_counts)))\n",
    "plt.title('Reason Group Counts (2000-2023) - Low Frequency (< 1,000,000)')\n",
    "plt.xlabel('Reason Group')\n",
    "plt.ylabel('Total Count')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This script analyzes water quality data across years, looking for inconsistencies \n",
    "# in parameter names and their corresponding short names and unit symbols.\n",
    "\n",
    "# Set the range of years for analysis\n",
    "years = range(2000, 2024)  # From 2000 to 2023\n",
    "\n",
    "# List to store inconsistencies from all years\n",
    "all_inconsistencies = []\n",
    "\n",
    "for year in years:\n",
    "    input_file_name = f\"{year} Water Quality Archive Cleaned.csv\"\n",
    "    \n",
    "    # Try to read the CSV file for the current year\n",
    "    try:\n",
    "        df = pd.read_csv(input_file_name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {input_file_name} not found. Skipping this year.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {input_file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Group the data by 'parameter_name' to check for inconsistencies\n",
    "    grouped = df.groupby('parameter_name')\n",
    "    inconsistencies = []\n",
    "\n",
    "    for name, group in grouped:\n",
    "        unique_shortnames = group['parameter_shortname'].unique()\n",
    "        if len(unique_shortnames) > 1:\n",
    "            # Collect unique unit symbols for each short name\n",
    "            unit_symbols = group.groupby('parameter_shortname')['unit_symbol'].unique().to_dict()\n",
    "            inconsistencies.append({\n",
    "                'parameter_name': name,\n",
    "                'parameter_shortnames': unique_shortnames,\n",
    "                'unit_symbols': unit_symbols\n",
    "            })\n",
    "\n",
    "    # Record inconsistencies if found\n",
    "    if inconsistencies:\n",
    "        print(f\"Inconsistencies found for year {year}:\")\n",
    "        for inc in inconsistencies:\n",
    "            print(f\"Parameter Name: {inc['parameter_name']}\")\n",
    "            for shortname, units in inc['unit_symbols'].items():\n",
    "                print(f\"  Shortname: {shortname}, Units: {units}\")\n",
    "        all_inconsistencies.extend(inconsistencies)\n",
    "    else:\n",
    "        print(f\"No inconsistencies found for year {year}.\")\n",
    "\n",
    "# Output all inconsistencies to a CSV file\n",
    "if all_inconsistencies:\n",
    "    # Create a DataFrame from the list of inconsistencies\n",
    "    df_inconsistencies = pd.DataFrame(all_inconsistencies)\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df_inconsistencies.to_csv(\"Inconsistencies Summary.csv\", index=False)\n",
    "    print(\"Inconsistencies summary saved to 'Inconsistencies Summary.csv'.\")\n",
    "else:\n",
    "    print(\"No inconsistencies found across all years.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a function to handle unit conversions\n",
    "def convert_units(value, from_unit, to_unit, parameter_name=None):\n",
    "    conversion_factors = {\n",
    "        ('mg/kg', 'µg/kg'): 1000,\n",
    "        ('µg/kg', 'mg/kg'): 0.001,\n",
    "        ('mg/l', 'µg/l'): 1000,\n",
    "        ('µg/l', 'mg/l'): 0.001,\n",
    "        ('g/kg', 'mg/l'): 1000,\n",
    "        ('µS/cm', 'mS/cm'): 0.001,\n",
    "        ('mS/cm', 'µS/cm'): 1000,\n",
    "        ('ng/l', 'µg/l'): 0.001,\n",
    "        ('µg/l', 'ng/l'): 1000,\n",
    "        ('NTU', 'FTU'): 1,\n",
    "        ('ppt', 'g/kg'): 1,\n",
    "        ('Ml/d', 'm³/s'): 0.011574,\n",
    "        ('m³/s', 'Ml/d'): 1 / 0.011574,\n",
    "        ('µg/l', 'pg/l'): 1_000_000,  # New conversion factor\n",
    "        ('pg/l', 'µg/l'): 1 / 1_000_000  # Reverse conversion\n",
    "    }\n",
    "    \n",
    "    if parameter_name:\n",
    "        # Special case for converting µg/l to µmol/l based on molecular weight\n",
    "        if from_unit == 'µg/l' and to_unit == 'µmol/l':\n",
    "            molecular_weights = {\n",
    "                'Phosphate as P': 31.0,  # Molecular weight for phosphate\n",
    "                # More molecular weights can be added as needed\n",
    "            }\n",
    "            if parameter_name in molecular_weights:\n",
    "                return value / molecular_weights[parameter_name]\n",
    "            else:\n",
    "                print(f\"Warning: No molecular weight provided for {parameter_name}\")\n",
    "                return None\n",
    "        \n",
    "        # Handling µg to µg/l conversion, assuming 1L volume\n",
    "        if from_unit == 'µg' and to_unit == 'µg/l':\n",
    "            return value  # Assuming volume of 1L\n",
    "    \n",
    "    # Standard conversion if rule exists\n",
    "    if (from_unit, to_unit) in conversion_factors:\n",
    "        return value * conversion_factors[(from_unit, to_unit)]\n",
    "    elif (to_unit, from_unit) in conversion_factors:\n",
    "        return value / conversion_factors[(to_unit, from_unit)]\n",
    "    else:\n",
    "        print(f\"Warning: No conversion rule for {from_unit} to {to_unit}\")\n",
    "        return None\n",
    "\n",
    "# Define the range of years to analyze\n",
    "years = range(2000, 2024)\n",
    "all_unit_differences = []\n",
    "\n",
    "# Step 1: Find parameters with different units in the same year and count the unit occurrences\n",
    "for year in years:\n",
    "    input_file_name = f\"{year} Water Quality Archive Cleaned.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_file_name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {input_file_name} not found. Skipping this year.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {input_file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    grouped = df.groupby('parameter_name')\n",
    "    unit_differences = []\n",
    "\n",
    "    for name, group in grouped:\n",
    "        unit_counts = group['unit_symbol'].value_counts()\n",
    "        if len(unit_counts) > 1:  # Check if multiple units exist for a parameter\n",
    "            unit_differences.append({\n",
    "                'parameter_name': name,\n",
    "                'unit_counts': unit_counts.to_dict()\n",
    "            })\n",
    "\n",
    "    if unit_differences:\n",
    "        all_unit_differences.extend(unit_differences)\n",
    "\n",
    "# Step 2 and 3: Convert minority unit values to the majority unit and update the dataset\n",
    "for year in years:\n",
    "    input_file_name = f\"{year} Water Quality Archive Cleaned.csv\"\n",
    "    output_file_name = f\"{year} Water Quality Archive Unified.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_file_name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {input_file_name} not found. Skipping this year.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {input_file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    for ud in all_unit_differences:\n",
    "        parameter_name = ud['parameter_name']\n",
    "        unit_counts = ud['unit_counts']\n",
    "        \n",
    "        majority_unit = max(unit_counts, key=unit_counts.get)  # Find the most common unit\n",
    "        \n",
    "        for unit in unit_counts:\n",
    "            if unit != majority_unit:\n",
    "                mask = (df['parameter_name'] == parameter_name) & (df['unit_symbol'] == unit)\n",
    "                # Special case for Cypermethrin to convert µg to µg/l\n",
    "                if parameter_name == 'Cypermethrin' and unit == 'µg':\n",
    "                    df.loc[mask, 'unit_symbol'] = 'µg/l'\n",
    "                else:\n",
    "                    df.loc[mask, 'sample_value'] = df.loc[mask, 'sample_value'].apply(convert_units, args=(unit, majority_unit, parameter_name))\n",
    "                    df.loc[mask, 'unit_symbol'] = majority_unit\n",
    "\n",
    "    df.to_csv(output_file_name, index=False)\n",
    "\n",
    "# Step 4: Verify that all units are now consistent for each parameter\n",
    "for year in years:\n",
    "    output_file_name = f\"{year} Water Quality Archive Unified.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(output_file_name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {output_file_name} not found. Skipping this year.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {output_file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    grouped = df.groupby('parameter_name')['unit_symbol'].nunique()\n",
    "    inconsistencies = grouped[grouped > 1]  # Check for parameters with multiple units\n",
    "\n",
    "    if not inconsistencies.empty:\n",
    "        print(f\"Verification failed for year {year}. Multiple units still exist for some parameters:\")\n",
    "        print(inconsistencies)\n",
    "    else:\n",
    "        print(f\"Verification successful for year {year}. All units are unified.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a dictionary to store MINEWATER station info for each year\n",
    "minewater_stations_by_year = {}\n",
    "\n",
    "# Loop through each year from 2000 to 2023\n",
    "for year in range(2000, 2023 + 1):\n",
    "    # Load data for the year\n",
    "    file_name = f'{year} Water Quality Archive Unified.csv'\n",
    "    data = pd.read_csv(file_name)\n",
    "    \n",
    "    # Filter for MINEWATER stations\n",
    "    minewater_data = data[data['station_type'].str.contains(\"MINEWATER\", case=False, na=False)]\n",
    "    \n",
    "    # Extract station number and geographical information (easting, northing)\n",
    "    minewater_stations = minewater_data[['station_number', 'easting', 'northing']].drop_duplicates()\n",
    "    \n",
    "    # Store the year's MINEWATER stations in the dictionary\n",
    "    minewater_stations_by_year[year] = minewater_stations\n",
    "\n",
    "# Initialize a dictionary to store comparison results\n",
    "comparison_results = {}\n",
    "\n",
    "# Compare MINEWATER station positions year by year\n",
    "for year in range(2000, 2022):  # Stop at 2022 to avoid index error\n",
    "    current_year_stations = minewater_stations_by_year[year]\n",
    "    next_year_stations = minewater_stations_by_year[year + 1]\n",
    "    \n",
    "    # Merge two years' data and label stations as added, removed, or unchanged\n",
    "    merged_stations = pd.merge(current_year_stations, next_year_stations, on=['station_number', 'easting', 'northing'], \n",
    "                               how='outer', indicator=True)\n",
    "    \n",
    "    # Count added, removed, and unchanged stations\n",
    "    added_stations = merged_stations[merged_stations['_merge'] == 'right_only'].shape[0]\n",
    "    removed_stations = merged_stations[merged_stations['_merge'] == 'left_only'].shape[0]\n",
    "    unchanged_stations = merged_stations[merged_stations['_merge'] == 'both'].shape[0]\n",
    "    \n",
    "    # Store the result in the dictionary\n",
    "    comparison_results[year] = {\n",
    "        'added': added_stations,\n",
    "        'removed': removed_stations,\n",
    "        'unchanged': unchanged_stations\n",
    "    }\n",
    "\n",
    "# Print or save the comparison results\n",
    "for year, result in comparison_results.items():\n",
    "    print(f\"From {year} to {year + 1}: Added = {result['added']}, Removed = {result['removed']}, Unchanged = {result['unchanged']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
